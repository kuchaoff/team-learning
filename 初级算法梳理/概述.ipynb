{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Task0-机器学习综述\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "## 机器学习分类\n",
    "### 基本分类\n",
    "\n",
    "1. 监督学习\n",
    "\n",
    "    监督学习是指利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。在监督学习的过程中会提供对错指示，通过不断地重复训练，使其找到给定的训练数据集中的某种模式或规律，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入和输出，主要应用于分类、回归任务。\n",
    "\n",
    "2. 非监督学习\n",
    "\n",
    "    与监督学习不同，在非监督学习中，无须对数据集进行标记，即没有输出。其需要从数据集中发现隐含的某种结构，从而获得样本数据的结构特征，判断哪些数据比较相似。因此，非监督学习目标不是告诉计算机怎么做，而是让它去学习怎样做事情。如聚类、关联分析等。\n",
    "\n",
    "3. 半监督学习\n",
    "\n",
    "    半监督学习是监督学习和非监督学习的结合，其在训练阶段使用的是未标记的数据和已标记的数据，不仅要学习属性之间的结构关系，也要输出分类模型进行预测。\n",
    "\n",
    "4. 强化学习\n",
    "\n",
    "    强化学习（Reinforcement Learning, RL），又称再励学习、评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题. \n",
    "    \n",
    "### 按模型分类\n",
    "\n",
    "1. 概率模型\n",
    "2. 非概率模型\n",
    "\n",
    "### 按算法分类\n",
    "\n",
    "1. 在线学习\n",
    "2. 批量学习\n",
    "\n",
    "### 按技巧学习\n",
    "\n",
    "1. 贝叶斯学习\n",
    "2. 核方法\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习模型\n",
    "\n",
    "机器学习 = 数据（data） + 模型（model） + 优化方法（optimal strategy）\n",
    "\n",
    "\n",
    "常见的机器学习算法\n",
    "\n",
    "1. Linear Algorithms\n",
    "   1. Linear Regression\n",
    "   2. Lasso Regression \n",
    "   3. Ridge Regression\n",
    "   4. Logistic Regression\n",
    "   5. Elastic NET\n",
    "   \n",
    "2. Decision Tree\n",
    "   1. ID3\n",
    "   2. C4.5\n",
    "   3. CART\n",
    "   4. CHAID\n",
    "   5. QUEST\n",
    "   \n",
    "3. SVM\n",
    "   1. linear kernel\n",
    "   2. RBF kernel\n",
    "   3. sigmoid kernel\n",
    "   \n",
    "4. Naive Bayes Algorithms\n",
    "   1. Naive Bayes\n",
    "   2. Gaussian Naive Bayes\n",
    "   3. Multinomial Naive Bayes\n",
    "   4. Bayesian Belief Network (BBN)\n",
    "   5. Bayesian Network (BN)\n",
    "   \n",
    "5. KNN\n",
    "\n",
    "6.  Clustering Algorithms\n",
    "    1.  k-Means\n",
    "    2.  k-Medians\n",
    "    3.  Expectation Maximisation (EM)\n",
    "    4.  Hierarchical Clustering\n",
    "    5.  DBSCAN\n",
    "\n",
    "7.  Random Forest\n",
    "\n",
    "8.  Dimensionality Reduction Algorithms\n",
    "    1. PCA\n",
    "    \n",
    "9. Gradient Boosting algorithms\n",
    "    1.  GBM\n",
    "    2.  XGBoost\n",
    "    3.  LightGBM\n",
    "    4.  CatBoost\n",
    "    \n",
    "10. Deep Learning Algorithms\n",
    "    1.  Convolutional Neural Network (CNN)\n",
    "    2.  Recurrent Neural Networks (RNNs)\n",
    "    3.  Long Short-Term Memory Networks (LSTMs)\n",
    "    4.  Stacked Auto-Encoders\n",
    "    5.  Deep Boltzmann Machine (DBM)\n",
    "    6.  Deep Belief Networks (DBN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 策略\n",
    "### 损失函数与风险函数\n",
    "\n",
    "**损失函数**度量模型**一次预测**的好坏，**风险函数**度量**平均意义**下模型预测的好坏。\n",
    "1. 损失函数(loss function)或代价函数(cost function)\n",
    "   损失函数定义为给定输入X的**预测值f(X)**和**真实值Y**之间的**非负实值**函数，记作L(Y,f(X))\n",
    "\n",
    "1. 风险函数(risk function)或期望损失(expected loss)\n",
    "   这个和模型的泛化误差的形式是一样的\n",
    "   $R_{exp}(f)=E_p[L(Y, f(X))]=\\int_{\\mathcal X\\times\\mathcal Y}L(y,f(x))P(x,y)\\, {\\rm d}x{\\rm d}y$\n",
    "   模型f(X)关于联合分布P(X,Y)的**平均意义下的**损失(**期望**损失)，但是因为P(X,Y)是未知的，所以前面的用词是**期望**，以及**平均意义下的**。\n",
    "\n",
    "   这个表示其实就是损失的均值，反映了对整个数据的预测效果的好坏，P(x,y)转换成\\frac {\\nu(X=x, Y=y)}{N}$更容易直观理解, 真实的数据N是无穷的。\n",
    "\n",
    "1. **经验风险**(empirical risk)或**经验损失**(empirical loss)\n",
    "   $R_{emp}(f)=\\frac{1}{N}\\sum^{N}_{i=1}L(y_i,f(x_i))$\n",
    "   模型f关于**训练样本集**的平均损失\n",
    "   根据大数定律，当样本容量N趋于无穷大时，经验风险趋于期望风险\n",
    "\n",
    "1. **结构风险**(structural risk)\n",
    "   $R_{srm}(f)=\\frac{1}{N}\\sum_{i=1}^{N}L(y_i,f(x_i))+\\lambda J(f)$\n",
    "   J(f)为模型复杂度, $\\lambda \\geqslant 0$是系数，用以权衡经验风险和模型复杂度。\n",
    "\n",
    "### 常见损失函数\n",
    "   \n",
    "1. 0-1损失函数\n",
    "$$\n",
    "L(y,f(x)) =\n",
    "\\begin{cases}\n",
    "0, & \\text{y = f(x)}  \\\\\n",
    "1, & \\text{y $\\neq$ f(x)}\n",
    "\\end{cases}\n",
    "$$\n",
    "2. 绝对值损失函数\n",
    "$$\n",
    "L(y,f(x))=|y-f(x)|\n",
    "$$\n",
    "3. 平方损失函数\n",
    "$$\n",
    "L(y,f(x))=(y-f(x))^2\n",
    "$$\n",
    "4. log对数损失函数\n",
    "$$\n",
    "L(y,f(x))=log(1+e^{-yf(x)})\n",
    "$$\n",
    "5. 指数损失函数\n",
    "$$\n",
    "L(y,f(x))=exp(-yf(x))\n",
    "$$\n",
    "6. Hinge损失函数\n",
    "$$\n",
    "L(w,b)=max\\{0,1-yf(x)\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习参数调优\n",
    "\n",
    "1. 网格搜索\n",
    "\n",
    "一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果\n",
    "\n",
    "2. 随机搜索\n",
    "\n",
    "与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。\n",
    "\n",
    "3. 贝叶斯优化算法\n",
    "\n",
    "贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习优化方法\n",
    "\n",
    "梯度下降是最常用的优化方法之一，它使用梯度的反方向$\\nabla_\\theta J(\\theta)$更新参数$\\theta$，使得目标函数$J(\\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新. \n",
    "1. (全量)梯度下降\n",
    "$$\n",
    "\\theta=\\theta-\\eta\\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "2. 随机梯度下降\n",
    "$$\n",
    "\\theta=\\theta-\\eta\\nabla_\\theta J(\\theta;x^{(i)},y^{(i)})\n",
    "$$\n",
    "3. 小批量梯度下降\n",
    "$$\n",
    "\\theta=\\theta-\\eta\\nabla_\\theta J(\\theta;x^{(i:i+n)},y^{(i:i+n)})\n",
    "$$\n",
    "4. 引入动量的梯度下降\n",
    "$$\n",
    "\\begin{cases}\n",
    "v_t=\\gamma v_{t-1}+\\eta \\nabla_\\theta J(\\theta)  \\\\\n",
    "\\theta=\\theta-v_t\n",
    "\\end{cases}\n",
    "$$\n",
    "5. 自适应学习率的Adagrad算法\n",
    "$$\n",
    "\\begin{cases}\n",
    "g_t= \\nabla_\\theta J(\\theta)  \\\\\n",
    "\\theta_{t+1}=\\theta_{t,i}-\\frac{\\eta}{\\sqrt{G_t+\\varepsilon}} \\cdot g_t\n",
    "\\end{cases}\n",
    "$$\n",
    "6. 牛顿法\n",
    "$$\n",
    "\\theta_{t+1}=\\theta_t-H^{-1}\\nabla_\\theta J(\\theta_t)\n",
    "$$\n",
    "其中:\n",
    "$t$: 迭代的轮数\n",
    "\n",
    "$\\eta$: 学习率\n",
    "\n",
    "$G_t$: 前t次迭代的梯度和\n",
    "\n",
    "$\\varepsilon:$很小的数,防止除0错误\n",
    "\n",
    "$H$: 损失函数相当于$\\theta$的Hession矩阵在$\\theta_t$处的估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习的评价指标\n",
    "1. MSE(Mean Squared Error)\n",
    "$$\n",
    "MSE(y,f(x))=\\frac{1}{N}\\sum_{i=1}^{N}(y-f(x))^2\n",
    "$$\n",
    "2. MAE(Mean Absolute Error)\n",
    "$$\n",
    "MSE(y,f(x))=\\frac{1}{N}\\sum_{i=1}^{N}|y-f(x)|\n",
    "$$\n",
    "3. RMSE(Root Mean Squard Error)\n",
    "$$\n",
    "RMSE(y,f(x))=\\frac{1}{1+MSE(y,f(x))}=MSE(y,f(x))^{0.5}\n",
    "$$\n",
    "4. Top-k准确率\n",
    "$$\n",
    "Top_k(y,pre_y)=\\begin{cases}\n",
    "1, {y \\in pre_y}  \\\\\n",
    "0, {y \\notin pre_y}\n",
    "\\end{cases}\n",
    "$$\n",
    "5. 混淆矩阵\n",
    "\n",
    "混淆矩阵|Predicted as Positive|Predicted as Negative\n",
    "|:-:|:-:|:-:|\n",
    "|Labeled as Positive|True Positive(TP)|False Negative(FN)|\n",
    "|Labeled as Negative|False Positive(FP)|True Negative(TN)|\n",
    "\n",
    "* 真正例(True Positive, TP):真实类别为正例, 预测类别为正例\n",
    "* 假负例(False Negative, FN): 真实类别为正例, 预测类别为负例\n",
    "* 假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 \n",
    "* 真负例(True Negative, TN): 真实类别为负例, 预测类别为负例\n",
    "\n",
    "* 真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数\n",
    "$$\n",
    "TPR=\\frac{TP}{TP+FN}\n",
    "$$\n",
    "* 假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数\n",
    "$$\n",
    "FNR=\\frac{FN}{TP+FN}\n",
    "$$\n",
    "\n",
    "* 假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，\n",
    "$$\n",
    "FPR=\\frac{FP}{FP+TN}\n",
    "$$\n",
    "* 真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，\n",
    "$$\n",
    "TNR=\\frac{TN}{FP+TN}\n",
    "$$\n",
    "* 准确率(Accuracy)\n",
    "$$\n",
    "ACC=\\frac{TP+TN}{TP+FN+FP+TN}\n",
    "$$\n",
    "* 精准率\n",
    "$$\n",
    "P=\\frac{TP}{TP+FP}\n",
    "$$\n",
    "* 召回率\n",
    "$$\n",
    "R=\\frac{TP}{TP+FN}\n",
    "$$\n",
    "* F1-Score\n",
    "$$\n",
    "\\frac{2}{F_1}=\\frac{1}{P}+\\frac{1}{R}\n",
    "$$\n",
    "* ROC\n",
    "\n",
    "ROC曲线的横轴为“假正例率”，纵轴为“真正例率”. 以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越靠左上角，分类器越佳. \n",
    "\n",
    "\n",
    "* AUC(Area Under Curve)\n",
    "\n",
    "AUC就是ROC曲线下的面积. 真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑. \n",
    "\n",
    "用AUC判断分类器（预测模型）优劣的标准:\n",
    "\n",
    "- AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.\n",
    "- 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.\n",
    "- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习模型选择\n",
    "\n",
    "1. 交叉验证\n",
    "\n",
    "所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。\n",
    "\n",
    "2. k-折叠交叉验证\n",
    "\n",
    "- 假设训练集为S ，将训练集等分为k份:$\\{S_1, S_2, ..., S_k\\}$. \n",
    "- 然后每次从集合中拿出k-1份进行训练\n",
    "- 利用集合中剩下的那一份来进行测试并计算损失值\n",
    "- 最后得到k次测试得到的损失值，并选择平均损失值最小的模型\n",
    "\n",
    "3. Bias与Variance，欠拟合与过拟合\n",
    "\n",
    "**欠拟合**一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.\n",
    "\n",
    "**过拟合**一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.\n",
    "\n",
    "\n",
    "4. 解决方法\n",
    "\n",
    "- 增加训练样本: 解决高Variance情况\n",
    "- 减少特征维数: 解决高Variance情况\n",
    "- 增加特征维数: 解决高Bias情况\n",
    "- 增加模型复杂度: 解决高Bias情况\n",
    "- 减小模型复杂度: 解决高Variance情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
